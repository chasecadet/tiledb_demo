{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ff2e2d9-cc0c-437d-a8ff-b609e0e9b4e7",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Kubeflow RAG Demo\n",
    "This demo walksthrough a simple RAG application using documents from the [introduction to AI/ML toolkits course](https://training.linuxfoundation.org/training/introduction-to-ai-ml-toolkits-with-kubeflow-lfs147/)\n",
    "This example has two main ways to handle building a Retrieval Augmented Generation application. The first is using a [custom predictor](https://kserve.github.io/website/0.8/modelserving/v1beta1/custom/custom_model/) and the second is using the [Nvidia NIM repo](https://catalog.ngc.nvidia.com/orgs/nim/teams/meta/containers/llama-2-7b-chat). In the future, we will discuss a third option using the [Kserve VLLM](https://docs.vllm.ai/en/latest/deployment/integrations/kserve.html) docs. At the time of authoring this demo, we were using a GCP install running Kubeflow 1.8 and the deployed Kserve version did not support the new VLLM instance. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5576b912-e03b-42b8-bdb5-980433944e82",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Requirements\n",
    "If you want to fully test the RAG application using a multi-tenant cluster, you will need to adjust the `VirtualService` that exposes the cluster and the `Gradio path` that the application expects to be served at. More details on that as we go, but you will need a container build environment and a registry to host those images. You can also build the other `Transformer` and `Predictor` images using the dockerfiles provided. Otherwise, you simply require a Kubeflow 1.8 cluster with Kserve 0.11 on it. If you are at a training, you should have already been granted access to a cluster. Worth noting, if you require a different `VirtualService` you will need to either have admin access to the cluster or send an admin the manifests to apply.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02d05805-bf1b-43f6-be08-6be015c9cffb",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Architecture and Flow\n",
    "### Step 1: Ingest Curated Documents Into An Object Store\n",
    "We use a [Kubeflow Notebook](https://www.kubeflow.org/docs/components/notebooks/) to ingest documents from our local document folder into [MinIO](https://min.io/). It is worth noting that due to AGPL requirements from MinIO, we are running an older version. Future updates will include distributed ingestion as well as multiple object store support (I.E. using the object store of choice for pipeplines). [SeaweedFS](https://github.com/seaweedfs/seaweedfs) has been considered as an option.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba2fd9aa-43e4-41f6-ba02-f33de701e0ad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install minio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe9a4c05-13ca-4885-9302-2863e6400b56",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from minio import Minio\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b3459a-53b5-4646-92b2-5358b4fed44b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "client = Minio(\"minio-service.kubeflow.svc.cluster.local:9000\",\n",
    "    access_key=\"minio\",\n",
    "    secret_key=\"minio123\",\n",
    "    secure=False,           \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b34c4107-4eaf-4da7-b760-48733c513b33",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "type(client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab0f344-dd47-4394-b423-a8543281cd84",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# List all buckets\n",
    "buckets = client.list_buckets()\n",
    "for bucket in buckets:\n",
    "    print(bucket.name, bucket.creation_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "330dd876-f15c-4554-bb13-5b423b679a89",
   "metadata": {},
   "source": [
    "Notice the `-kfp` bucket. This bucket is used for marshalling with Kubeflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c131346c-3a9c-452c-bd01-968d3ac98b9e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#change this to whatever you'd like if using a multi-tenant environment. \n",
    "#This is a shared MinIO, so you will overwrite each others document storage if you fail to do so.\n",
    "bucket_name = \"sanfranai\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d328e660-6ee9-4cc2-9538-f1ad5704f9f6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# List objects in the bucket\n",
    "# This will error out if the bucket doesn't exist with \"The specified bucket does not exist\"\n",
    "objects = client.list_objects(bucket_name, recursive=True)\n",
    "for obj in objects:\n",
    "    print(obj.object_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95bcbd88-0231-48d3-a403-6804a8d2bfa8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "client.bucket_exists(bucket_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "303f134d-a8f3-4fe7-aba5-badcae41a2a0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def upload_files(bucket_name, file_location, client):\n",
    "    found = False  # Initialize 'found' before the try block\n",
    "    print(\"Current working directory:\", os.getcwd())\n",
    "    print(\"Listing directories in the current working directory:\", os.listdir(\".\"))\n",
    "    print(f\"Checking existence of {file_location}: \", os.path.exists(file_location))\n",
    "\n",
    "    try:\n",
    "        found = client.bucket_exists(bucket_name)\n",
    "    except Exception as e:\n",
    "        print(\"error trying to search for MinIO Bucket:\", e)\n",
    "        return  # Return early since we cannot proceed without knowing if the bucket exists\n",
    "\n",
    "    if not found:\n",
    "        try:\n",
    "            client.make_bucket(bucket_name)\n",
    "            print(\"Created bucket\", bucket_name)\n",
    "        except Exception as e:\n",
    "            print(\"Failed to create bucket:\", e)\n",
    "            return  # Return early since we cannot proceed if the bucket cannot be created\n",
    "    else:\n",
    "        print(\"Bucket\", bucket_name, \"exists, we won't attempt to create one\")\n",
    "        \n",
    "    # Ensure the directory exists\n",
    "    if not os.path.isdir(file_location):\n",
    "        print(f\"The directory {file_location} does not exist.\")\n",
    "        return\n",
    "\n",
    "    # Iterate through all files in the directory\n",
    "    for file_name in os.listdir(file_location):\n",
    "        # Construct the full file path\n",
    "        source_file = os.path.join(file_location, file_name)\n",
    "        # Check if it's a file and not a directory\n",
    "        if os.path.isfile(source_file):\n",
    "            try:\n",
    "                # Upload the file\n",
    "                client.fput_object(bucket_name, file_name, source_file)\n",
    "                print(f\"Successfully uploaded {file_name} to bucket {bucket_name}.\")\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to upload {file_name}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf1c57d-8fc3-4147-bdd1-6f579587984a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "upload_files(bucket_name,\"./documentation\",client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311073e5-4ee5-4165-a59a-03df79b5a876",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# List objects in the bucket\n",
    "objects = client.list_objects(bucket_name, recursive=True)\n",
    "for obj in objects:\n",
    "    print(obj.object_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2efdfddf-21c9-4703-a1b3-b8544287b96e",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Step 2: Deploy Vector Store and Inferencing Container (TileDB) \n",
    "The next step is to use [TileDB Vector Search + Langchain](https://github.com/TileDB-Inc/TileDB-Vector-Search) to build a vector store. We will serve the vector store as an `InferenceService`. The `InferenceService` will ingest the data from `MinIO` and start the `TileDB` vector database. The `InferenceService` uses `all-MiniLM-L6-v2` for embeddings. Worth noting this is good for some generic RAG tasks, but for more specialized RAG workflows (think Life Sciences), you will need a specialized embedding model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c36469-5a5f-48d9-bdf5-32d982ddb21f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f35f0324-b068-41f6-86d7-0aae90c3e9a6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Assuming /home/jovyan/tiledb_demo/notebooks as the directory \n",
    "# If you are in a multi-tenant cluster, please make sure you view the manifest and update it with your desired values. \n",
    "!kubectl apply -f ../manifests/core/minio_secret_key.yml\n",
    "!kubectl apply -f ../manifests/core/vector_db_isvc.yml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a9f4c2-66e8-4681-a55a-eb3594cb295f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!kubectl get inferenceservices vectorstore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab01825-dac0-4e82-8d26-e841c8df1186",
   "metadata": {
    "tags": []
   },
   "source": [
    "Wait for the above output to be `True` for `Ready`. This can take up to 5 minutes.  You should also see a URL for the vector store. If you are using your own images, you will need to update the `vector_db_isvc.yml` to use your image and ensure your Kubeflow cluster has [access to the registry](https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/). Once the VectorStore is up and running, you can run `kubectl get pods` and find a pod with a name similar to `vectorstore-predictor-00001-deployment-`. Notice the pod has created an `in-memory` index. [TileDB](https://python.langchain.com/docs/integrations/vectorstores/tiledb/) CAN run using object as backing to store the vector embeddi!kubectl get inferenceservices vectorstorengs (these are 1D arrays), but we are using in-memory for simplicity for this first iteration. Once the `InferenceService` is up and happy, run the below cell to validate you can get a prediction. Make sure to add your `namespace` in the proper spot below. Use the URL from the `!kubectl get inferenceservices vectorstore` command above. Once complete, run the below command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e5db8c5-b83c-493b-af9a-f85eacd4a034",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = {\n",
    "  \"instances\": [{\n",
    "    \"input\": \"When was Kubeflow open sourced?\",\n",
    "    \"num_docs\": 6  # number of documents to retrieve\n",
    "  }]\n",
    "}\n",
    "\n",
    "URL = \"http://vectorstore-predictor.christensenc3526.svc.cluster.local/v1/models/vectorstore:predict\"  # Adjust path as necessary\n",
    "\n",
    "response = requests.post(URL, json=data, verify=False)  # 'verify=False' for self-signed certs\n",
    "#print(response)\n",
    "#print(response.json())\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb53358-f2b9-4aad-b015-88e932ce1417",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Step 3: Deploy a Custom Model and Transformer\n",
    "This section will deploy a custom `predictor` and a `transformer` for our end user application to use.\n",
    "We will serve a `orca-mini-3b` model for generation and the `transformer` to retrieve the documents and provide the context from the vector store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a96742-5f61-47de-bbc0-1e3e3e63d45a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!kubectl apply -f ../manifests/Inference/CPU/llm_isvc_custom.yml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93da9cb5-eab0-4d07-8980-cee66d64918e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!kubectl get inferenceservice  llm "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa0d7092-b005-4ffb-bc4d-b7054b5862a9",
   "metadata": {},
   "source": [
    "Once the above line reports `READY` `True`, adjust below to be your namespace and run the command. You should see an `llm` model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f40d452-29cf-4693-9a91-8955bae576a4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!curl -X GET http://llm-predictor.christensenc3526.svc.cluster.local/v1/models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1385b7cc-fea9-4288-946e-305869b98748",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!curl -X GET http://llm.christensenc3526.svc.cluster.local/v1/models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd8b7efe-c22d-4780-9a9b-e2db42206017",
   "metadata": {},
   "source": [
    "We can now test the model by sending a prediction! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ccdba62-9ab0-4ee9-abf9-6fcce8b12002",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "URL = \"http://llm-transformer.christensenc3526.svc.cluster.local/v1/models/llm:predict\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36018a97-9a79-4c28-963e-8e92674a45b2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = {\n",
    "  \"instances\": [{\n",
    "      \"system\": \"You are an AI assistant. You will be given a task. You must generate a detailed answer.\",\n",
    "      \"instruction\": \"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\",\n",
    "      \"input\": \"What is Kubeflow?\",\n",
    "      \"max_tokens\": 5000,\n",
    "      \"top_k\": 100,\n",
    "      \"top_p\": 0.4,\n",
    "      \"num_docs\": 3,\n",
    "      \"temperature\": 0.2\n",
    "  }]\n",
    "}\n",
    "response = requests.post(URL, json=data,verify=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c6db0f-fe3c-4c25-8cee-da734a3d3fc4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(response)\n",
    "#print(response.json())\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f22de2d4-58fb-44d8-be90-b4dd516971dd",
   "metadata": {},
   "source": [
    "Note that the above is running on a CPU so its gonna be SLOW. We are going to fix that issue when we use Nvidia to deploy a Llama model. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b96fb41-287f-446d-9cdb-c63c850096e8",
   "metadata": {},
   "source": [
    "### Step 4: Deploying the Frontend Application"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aee031a-97d3-4909-9d14-7d40ae2eba66",
   "metadata": {},
   "source": [
    "The frontend application is [Gradio](https://www.gradio.app/) served from a [Kubernetes deployment](https://kubernetes.io/docs/concepts/workloads/controllers/deployment/). The manifest by default serves the frontend at `/professorflow`. This is also configured at the Gradio level. If you are in a multi-tenant environment, you will need to adjust Gradio's configuration at `tiledb_demo/dockerfiles/frontends/frontend/src/app.py` and then build and push the container to a registry. You will then need to update  `tiledb_demo/manifests/core/frontend/frontend.yml` with the appropriate image. \n",
    "The `frontend.yml` has several manifests within it. \n",
    "1. `demo-deployment` is a Kubernetes deployment that will deploy our frontend application\n",
    "2. `demo` is a [service](https://kubernetes.io/docs/concepts/services-networking/service/) for routing requests to your deployment container. \n",
    "3. `frontend-virtual-service` is a [virtual service](https://istio.io/latest/docs/reference/config/networking/virtual-service/) that will ensure we can route requests to our cluster using [Istio](https://istio.io). \n",
    "4. `demo-service-external-access` is an [AuthorizationPolicy](https://istio.io/latest/docs/reference/config/security/authorization-policy/) that will allow access externally to our demo service. \n",
    "5.  **This manifest requires admin access**. \n",
    "\n",
    "Before applying the manifests, ensure they reflect the path where you are intending to serve `gradio` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e360b4be-ddbc-49e9-a160-71db8d80d249",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!kubectl apply -f ../manifests/core/frontend/frontend.yml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecbf2655-8234-4204-92ac-c35ed61e13f6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!kubectl get deployment demo-deployment "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97cdbf72-f275-413d-b959-436bbfe22555",
   "metadata": {},
   "source": [
    "Once the above shows `READY 1/1` Visit the URL your application is being served at (example is https://kubeflow.endpoints.sanfranai25.cloud.goog/professorflow). You should see a Gradio interface! Now, enter a prompt asking questions about Kubeflow and wait! Note: this will take AWHILE due to us using CPU for inferencing, but we will fix this! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9d5738b-a9c2-4b11-9ba9-49a6333d5949",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Serving a LLAMA-2-7b-chat with NVIDIA NIM\n",
    "We will be serving a Llama model, using Kserve. We will update our `InferenceService` to serve a new custome `transformer` and route to the NVIDIA `predictor`. This will improve our inferencing times. The detailed instructions can be found [here](https://github.com/NVIDIA/nim-deploy/blob/main/kserve/README.md). You will need to request an `NGC_API_KEY` to create a [Kubernetes secret](https://kubernetes.io/docs/concepts/configuration/secret/) with the NGC_API_KEY as well as a registry secret. Details on how to do that [here](https://docs.nvidia.com/ai-enterprise/deployment/spark-rapids-accelerator/latest/appendix-ngc.html). You will also need to deploy a valid [Nvidia Plugin](https://github.com/NVIDIA/k8s-device-plugin). If using GCP, you may need to reference [this guide](https://github.com/GoogleCloudPlatform/container-engine-accelerators/issues/356) if using L4 GPUs. You can (check catalog)[https://catalog.ngc.nvidia.com/orgs/nim/teams/meta/containers/llama-3.1-8b-instruct] for valid NVIDIA images. \n",
    "\n",
    "You will need to deploy:\n",
    "1. An `inference-pvc` to store the model. \n",
    "2. A [llm server runtime](https://kserve.github.io/website/0.8/modelserving/servingruntimes/)\n",
    "3. The updated `InferenceService` with a new transformer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "260fa605-4275-4372-aae9-4a6a61913581",
   "metadata": {},
   "outputs": [],
   "source": [
    "!export NGC_API_KEY=..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb562d45-9f43-4950-9b69-0ad2d361c27a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!kubectl create secret generic nvidia-nim-secrets   --from-literal=NGC_API_KEY=\"$NGC_API_KEY\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb432aa-e38a-4952-8eee-e3d4c33e5591",
   "metadata": {},
   "outputs": [],
   "source": [
    "!kubectl create secret docker-registry ngc-secret -n admin --docker-server=nvcr.io --docker-username='$oauthtoken' --docker-password=${NGC_API_KEY}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "233bcf1e-24a8-480a-8925-28e88f54f016",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inferenceservice.serving.kserve.io/llm unchanged\n",
      "persistentvolumeclaim/nvidia-pvc unchanged\n",
      "servingruntime.serving.kserve.io/llama-2-7b-chat unchanged\n"
     ]
    }
   ],
   "source": [
    "!kubectl apply -f ../manifests/Inference/GPU/nvidia-inference/llm_inference_nvidia.yml\n",
    "!kubectl apply -f ../manifests/Inference/GPU/nvidia-inference/inference-pvc.yaml\n",
    "!kubectl apply -f ../manifests/Inference/GPU/nvidia-inference/llm_server_runtime_nvidia.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "81c8de76-bc2e-4ddb-bf68-77c84e181e1d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME   URL                                                                     READY   PREV   LATEST   PREVROLLEDOUTREVISION   LATESTREADYREVISION   AGE\n",
      "llm    http://llm.christensenc3526.kubeflow.endpoints.sanfranai25.cloud.goog   False          100                              llm-predictor-00001   24h\n"
     ]
    }
   ],
   "source": [
    "!kubectl get inferenceservice llm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04323baf-fd2b-4522-91e3-b0fdd6bc5f68",
   "metadata": {
    "tags": []
   },
   "source": [
    "# run this command from your terminal once the above command says READY True. Update below with your namespace. \n",
    "# NOTE: The container can take awhile to be ready. \n",
    "```\n",
    "curl -X POST http://llm-predictor.christensenc3526.svc.cluster.local/v1/chat/completions \\\n",
    "  -H \"Content-Type: application/json\" \\\n",
    "  -d '{\n",
    "    \"model\": \"meta/llama-2-7b-chat\",\n",
    "    \"messages\": [{\"role\": \"user\", \"content\": \"What is Kubeflow?\"}],\n",
    "    \"temperature\": 0.5,\n",
    "    \"top_p\": 1,\n",
    "    \"max_tokens\": 256,\n",
    "    \"stream\": false\n",
    "  }'\n",
    "``` "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a10c6b-e434-40fb-afb5-87f58b9ed5ca",
   "metadata": {},
   "source": [
    "Now, use the same Gradio app, and run a query! You should notice a quicker response and higher quality response (if you have GPU nodes).Delete the `inferenceservice`and your cluster should scale down. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8005c278-1687-4e88-a819-8f2944b03a1a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
